{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1.  Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mp\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Actions(x_pos,y_pos):           \n",
    "    x_max = 4                                \n",
    "    y_max = 3\n",
    "    directions = np.array([0,0,0,0,1])      #[\"Up\", \"Left\", \"Down\", \"Right\", \"DoNothing\"]\n",
    "    if((x_pos >=0 and x_pos <=x_max) and (y_pos >=0 and y_pos <=y_max)):  \n",
    "        if(y_pos-0)>0:                    \n",
    "            directions[0] = 1\n",
    "        if(x_pos-0)>0:\n",
    "            directions[1] = 1\n",
    "        if(y_max-y_pos)>0:\n",
    "            directions[2] = 1\n",
    "        if(x_max-x_pos)>0:\n",
    "            directions[3] = 1\n",
    "        return directions\n",
    "    else:\n",
    "        print(\"Error in direction values\")        \n",
    "        \n",
    "def calculate(g_x,j_n):                          \n",
    "    alpha = 0.9900\n",
    "    ans = g_x +alpha*(j_n)\n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_world = [[0,0,1,0,-1],               #grey box is np.inf\n",
    "              [0,np.inf,1,0,10],\n",
    "              [0,10,np.inf,1,0],\n",
    "              [1,0,0,10,-1]]\n",
    "\n",
    "prev_world = np.array(color_world)\n",
    "color_world = np.array(color_world)           #this is the main version updated\n",
    "world_copy = np.copy(color_world)             # keep original version of color_world\n",
    "\n",
    "def VI(color_world):\n",
    "    change = 3                              #Arbitrary value used to initialize \"change\"\n",
    "    runtime = 0                           #number of runs\n",
    "    threshold = 0.000001\n",
    "\n",
    "    policy = color_world.tolist()         #initialize control matrix\n",
    "    while change > threshold:    \n",
    "        prev_world = np.copy(color_world)         #get previous iteration values\n",
    "        for i in range(len(color_world)):           #y iteration\n",
    "            for j in range(len(color_world[0])):    #x iteration\n",
    "                if color_world[i][j] != np.inf:      # this line is just to avoid calculations with np.inf\n",
    "                    dr = [\"GoUp\", \"GoLeft\", \"GoDown\", \"GoRight\", \"DoNothing\"] #\n",
    "                    directions = Actions(j,i)      #x,y  \n",
    "                    \n",
    "                    #calculate for DoNothing\n",
    "                    val_hold = np.zeros(5)\n",
    "                    value = calculate(world_copy[i][j],prev_world[i][j])   \n",
    "                    val_hold[4] = value          \n",
    "                    \n",
    "                    # calculate value for Up\n",
    "                    if (directions[0] == 1):     \n",
    "                        value = calculate(world_copy[i][j],prev_world[i-1][j])\n",
    "                        val_hold[0]= value\n",
    "                        \n",
    "                    # calculate value for Left\n",
    "                    if (directions[1] == 1):     \n",
    "                        value = calculate(world_copy[i][j],prev_world[i][j-1])\n",
    "                        val_hold[1] = value\n",
    "                        \n",
    "                    # calculate value for Down\n",
    "                    if (directions[2] == 1):     \n",
    "                        value = calculate(world_copy[i][j],prev_world[i+1][j])\n",
    "                        val_hold[2] = value\n",
    "                        \n",
    "                    # calculate value for Right    \n",
    "                    if (directions[3] == 1):     \n",
    "                        value = calculate(world_copy[i][j],prev_world[i][j+1])\n",
    "                        val_hold[3] = value \n",
    "                    \n",
    "                    #find the min value\n",
    "                    min_val = min(val_hold)        \n",
    "                    if min_val == val_hold[4]:\n",
    "                        policy[i][j] = dr[4]  \n",
    "                    else:\n",
    "                        ind = np.where(val_hold == min_val)\n",
    "                        policy[i][j] = dr[ind[0][0]]\n",
    "                        \n",
    "                    color_world[i][j] = min_val     \n",
    "        runtime += 1     \n",
    "        change = abs(color_world[i][j])-abs(prev_world[i][j])\n",
    "    \n",
    "    policy = np.array(policy)\n",
    "        \n",
    "    print(\"\\n The original cost values:\\n\",world_copy)\n",
    "    print(\"\\n Optimal Policy:\\n\",policy)\n",
    "    print(\"\\n Value Function:\\n\",color_world)\n",
    "    print(\"\\n The number of iterations for convergence:\",runtime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The original cost values:\n",
      " [[ 0.  0.  1.  0. -1.]\n",
      " [ 0. inf  1.  0. 10.]\n",
      " [ 0. 10. inf  1.  0.]\n",
      " [ 1.  0.  0. 10. -1.]]\n",
      "\n",
      " Optimal Policy:\n",
      " [['GoRight' 'GoRight' 'GoRight' 'GoRight' 'DoNothing']\n",
      " ['GoUp' 'inf' 'GoRight' 'GoUp' 'GoUp']\n",
      " ['GoUp' 'GoLeft' 'inf' 'GoRight' 'GoDown']\n",
      " ['GoUp' 'GoLeft' 'GoLeft' 'GoRight' 'DoNothing']]\n",
      "\n",
      " Value Function:\n",
      " [[-95.07940237 -96.03980137 -97.00990137 -98.99990137 -99.99990137]\n",
      " [-94.12860736          inf -96.02980137 -98.00990137 -88.99990137]\n",
      " [-93.1873203  -82.25544611          inf -97.00990137 -98.99990137]\n",
      " [-91.25544611 -90.34289066 -89.43946077 -88.99990137 -99.99990137]]\n",
      "\n",
      " The number of iterations for convergence: 1375\n"
     ]
    }
   ],
   "source": [
    "VI(color_world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Policy Iteration v/s Value Iteration\n",
    "\n",
    "In case of Policy iteration, we perform policy evaluation and policy improvement, and these two are repeated iteratively until policy converges.\n",
    "In case of Value iteration includes, we find optimal value function and do one policy extraction. There is no repeat of the two because once the value function is optimal, then the policy out of it should also be optimal (i.e. converged).\n",
    "Thse algorithms have similar approach, but policy iteration is faster than value iteration with time complexity of $O(S^2)$ per iteration,and converges more quickly than a value function. Value Iteration is slow â€“ $O(S^2A)$.\n",
    "Value iteration is simpler but its computationally heavy. Whereas, Policy iteration is complicated but its computationally cheap w.r.t value iteration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
